{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentMining.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"TlLeVJ1tv5fQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"5738f602-df22-439f-dccb-fcb8f788b89d","executionInfo":{"status":"ok","timestamp":1541612485095,"user_tz":-360,"elapsed":4592,"user":{"displayName":"Habib Rahman","photoUrl":"https://lh5.googleusercontent.com/-qa4jTjcWaIQ/AAAAAAAAAAI/AAAAAAAABU8/JtMtsPIrId0/s64/photo.jpg","userId":"10021246046723718031"}}},"cell_type":"code","source":["!pip2 install numpy\n","\n","#!python2 \"/content/drive/My Drive/Research/Sent. Mining/mov.py\""],"execution_count":25,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.14.6)\n"],"name":"stdout"}]},{"metadata":{"id":"rFbGZQXHpxY2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"e0f00eb5-5290-4be2-902f-1ae51034c78f","executionInfo":{"status":"ok","timestamp":1541612504354,"user_tz":-360,"elapsed":9495,"user":{"displayName":"Habib Rahman","photoUrl":"https://lh5.googleusercontent.com/-qa4jTjcWaIQ/AAAAAAAAAAI/AAAAAAAABU8/JtMtsPIrId0/s64/photo.jpg","userId":"10021246046723718031"}}},"cell_type":"code","source":["#from google.colab import drive\n","#drive.mount('/content/drive/')\n","#!python2 '/content/drive/My Drive/Research/Sent.Mining/mov.py'\n","#with open('/content/drive/My Drive/Research/Sent.Mining/foo.txt', 'w') as f:\n","#  f.write('Hello Google Drive!')\n","#!cat /content/drive/My\\ Drive/Research/Sent.Mining/foo.txt"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/content/drive/My Drive/Research/Sent.Mining/mov.py\", line 103, in <module>\n","    for train_index, test_index in kf.split(corpus,labels):\n","  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_split.py\", line 324, in split\n","    X, y, groups = indexable(X, y, groups)\n","  File \"/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py\", line 229, in indexable\n","    check_consistent_length(*result)\n","  File \"/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py\", line 204, in check_consistent_length\n","    \" samples: %r\" % [int(l) for l in lengths])\n","ValueError: Found input variables with inconsistent numbers of samples: [2010, 2000]\n"],"name":"stdout"}]},{"metadata":{"id":"A_6HDhZIRIph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":499},"outputId":"ef16bbc2-6a96-4430-8fe6-9bbb6a6b5220","executionInfo":{"status":"error","timestamp":1541620947891,"user_tz":-360,"elapsed":8741,"user":{"displayName":"Habib Rahman","photoUrl":"https://lh5.googleusercontent.com/-qa4jTjcWaIQ/AAAAAAAAAAI/AAAAAAAABU8/JtMtsPIrId0/s64/photo.jpg","userId":"10021246046723718031"}}},"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Feb 08 17:03:30 2017\n","\n","@author: Habibur Rahman\n","\"\"\"\n","\n","import os\n","import numpy as np\n","#from google.colab import drive\n","from numpy import sort\n","from sklearn.metrics import confusion_matrix\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","h = .02  # step size in the mesh\n","\n","def make_Corpus(root_dir):\n","    polarity_dirs = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    \n","    corpus = []    \n","  \n","    for polarity_dir in polarity_dirs:\n","        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n","        for review in reviews:\n","            doc_string = \"\";\n","            with open(review) as rev:\n","                for line in rev:\n","                    doc_string = doc_string + line\n","            if not corpus:\n","                corpus = [doc_string]\n","            else:\n","                corpus.append(doc_string)\n","    #print \"Corpus\\n\",corpus\n","    return corpus\n","\n","  \n","#drive.mount('/content/drive/')\n","  \n","#Create a dictionary of words with its frequency\n","\n","root_dir = '/content/drive/My Drive/Research/Sent.Mining/txt_sentoken'\n","corpus = make_Corpus(root_dir)\n","\n","\n","#Prepare feature vectors per training mail and its labels\n","labels = np.zeros(2000);\n","labels[0:1000]=0;\n","labels[1000:2000]=1; \n","      \n","kf = StratifiedKFold(n_splits=10)\n","\n","\n","\n","names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Logistic Regression\", #\"Gaussian Process\",\n","         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n","         \"Naive Bayes\", \"QDA\"]\n","\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"linear\", C=0.025),\n","    SVC(gamma=2, C=1),\n","    LogisticRegression(),\n","    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n","    DecisionTreeClassifier(max_depth=5),\n","    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n","    MLPClassifier(alpha=1),\n","    AdaBoostClassifier(),\n","    GaussianNB(),\n","    QuadraticDiscriminantAnalysis()]\n","\n","scaler=MinMaxScaler(copy=True, feature_range=(0, 1))\n","\n","#TSVD   start from here\n","svd=TruncatedSVD(n_components=200,n_iter=5,random_state=42)\n","#svd=NMF(n_components=50)\n","normalizer = Normalizer(copy=False)\n","lsa = make_pipeline(svd, normalizer)\n","#after normalize\n","vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True,stop_words='english')\n","corpus = vectorizer.fit_transform(corpus)\n","\n","#fit corpus using tsvd\n","corpus=lsa.fit_transform(corpus)\n","corpus=scaler.fit_transform(corpus)\n","\n","for name, model in zip(names, classifiers):\n","    total = 0\n","    totalMat = np.zeros((2,2));\n","    for train_index, test_index in kf.split(corpus,labels):\n","        y_train, y_test = labels[train_index], labels[test_index]\n","        X_train = [corpus[i] for i in train_index]\n","        X_test = [corpus[i] for i in test_index]\n","        Total_Train_data = len(X_train)\n","        Total_Test_data = len(X_test)\n","        model.fit(X_train, y_train)\n","        result = model.predict(X_test)\n","        totalMat = totalMat + confusion_matrix(y_test, result, labels=[0,1])\n","        total = total+sum(y_test==result)\n","        thresholds = sort(XGBClassifier().feature_importances_)\n","        for thresh in thresholds:\n","          # select features using threshold\n","          selection = SelectFromModel(AdaBoostClassifier(), threshold=thresh, prefit=True)\n","          select_X_train = selection.transform(X_train)\n","          # train model\n","          selection_model = XGBClassifier()\n","          selection_model.fit(select_X_train, y_train)\n","          # eval model\n","          select_X_test = selection.transform(X_test)\n","          y_pred = selection_model.predict(select_X_test)\n","          predictions = [round(value) for value in y_pred]\n","          accuracy = accuracy_score(y_test, predictions)\n","          print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n","    print \"\\n\"+name+\":\\n\";\n","    print \"Confusion matrix:\\n\",totalMat    \n","    print \"performance:\", (total/2000.0)*100.0"],"execution_count":11,"outputs":[{"output_type":"error","ename":"XGBoostError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mXGBoostError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-11-829eb1eeeb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtotalMat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalMat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0;31m# select features using threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \"\"\"\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mget_booster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need to call fit beforehand'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXGBoostError\u001b[0m: need to call fit beforehand"]}]},{"metadata":{"id":"H-qkFYC0iQwo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"0c9247d8-de57-4b14-ed0c-7b156f6d921d","executionInfo":{"status":"ok","timestamp":1541617800908,"user_tz":-360,"elapsed":3892,"user":{"displayName":"Habib Rahman","photoUrl":"https://lh5.googleusercontent.com/-qa4jTjcWaIQ/AAAAAAAAAAI/AAAAAAAABU8/JtMtsPIrId0/s64/photo.jpg","userId":"10021246046723718031"}}},"cell_type":"code","source":[""],"execution_count":33,"outputs":[{"output_type":"stream","text":["total 8\n","drwx------ 3 root root 4096 Nov  7 16:45 drive\n","drwxr-xr-x 2 root root 4096 Nov  5 19:58 sample_data\n"],"name":"stdout"}]}]}